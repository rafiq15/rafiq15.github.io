<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <title>Apache Kafka</title>
    <link rel="stylesheet" href="../style.css" />
</head>

<body>
    <header>
        <h1>Apache Kafka ‚Äì When to Use What</h1>
        <p><em>Published: May 2025</em></p>
    </header>
    <nav>
        <a href="../blog.html">&larr; Back to Blog</a>
    </nav>
    <main>
        <h2>What is Apache Kafka</h2>
        <p>
            Kafka is an event streaming platform, because it is designed to handle and process continuous streams of
            events or data in real time.
        </p>
        <h2>When to use</h2>
        <p>
            Apache Kafka is a versatile platform that can be used in various scenarios where high-throughput,
            low-latency, and real-time data processing are required. Here are some common use cases and situations where
            Kafka is particularly well-suited:
        </p>
        <ol>
            <li><strong>Real-time Data Streaming:</strong> Kafka is ideal for applications that require real-time data
                processing, such as log aggregation, event sourcing, and stream processing.</li>
            <li><strong>Data Integration:</strong> Kafka can serve as a central hub for integrating data from multiple
                sources, allowing you to collect, transform, and distribute data across different systems.</li>
            <li><strong>Event-Driven Architectures:</strong> Kafka is commonly used in event-driven architectures to
                decouple services and enable asynchronous communication between them.</li>
            <li><strong>Analytics and Monitoring:</strong> Kafka can be used to collect and analyze large volumes of
                data in real time, making it suitable for monitoring applications and analytics pipelines.</li>
        </ol>
        <h2>Kafka Components</h2>
        <ol>
            <li><strong>Producers:</strong> Applications that publish data to Kafka topics. The Producers is a source of
                data while publish messages/events.</li>
            <li><strong>Consumers:</strong> Applications that subscribe to topics and process the data. Consumers act as
                a receiver. It's responsible to receive/consume message/events.</li>
            <li><strong>Topics:</strong> Categories or feeds to which records are published.</li>
            <li><strong>Brokers:</strong> The Kafka Brokers is nothing but just a server that stores and manages the
                data. It works as an intermediary (helps in message exchanges) between producers and consumers.</li>
            <li><strong>Clusters:</strong> A group of Kafka brokers that work together to provide high availability and
                fault tolerance. There can be one or more brokers in the Kafka Clusters.</li>
            <li><strong>Zookeeper:</strong> A service for coordinating distributed applications, used by Kafka for
                managing brokers and topics.</li>
            <li><strong>Partitions:</strong> Subdivisions of topics that allow for parallel processing and scalability.
            </li>
            <li><strong>Offsets:</strong> Unique identifiers for each record within a partition, used to track the
                position of consumers.</li>
            <li><strong>Consumer Groups:</strong> A group of consumers that work together to consume data from a topic,
                allowing for load balancing and fault tolerance.</li>
            <li><strong>Streams API:</strong> A library for building stream processing applications that can read from
                and write to Kafka topics.</li>
            <li><strong>Connect API:</strong> A framework for integrating Kafka with external systems, such as databases
                and message queues.</li>
            <li><strong>Schema Registry:</strong> A service for managing and enforcing data schemas in Kafka, ensuring
                data compatibility and consistency.</li>
        </ol>
        <h2>How Kafka Partitions Work by Default</h2>
        <p>
            By default, Kafka partitions messages in a round-robin fashion across all available partitions for a topic.
            This means that when a producer sends messages to a topic, the messages are distributed evenly across the
            partitions, allowing for parallel processing and increased throughput.
        </p>
        <p>
            However, Kafka also allows for more advanced partitioning strategies. Producers can specify a partition key
            when sending messages, which determines the partition to which the message is sent. This is useful for
            ensuring that related messages are sent to the same partition, allowing for ordered processing.
        </p>
        <p>
            <strong>Partition Assignment:</strong> When you create a topic, you specify the number of partitions. Kafka
            clients (producers and consumers) automatically interact with these partitions to distribute or consume
            data.
        </p>
        <p>
            <strong>Load Distribution:</strong> Kafka automatically distributes partitions across brokers in the cluster
            to ensure load balancing. This helps to prevent any single broker from becoming a bottleneck and allows for
            better resource utilization.
        </p>
        <p>
            <strong>Message Routing:</strong> Producers send messages to partitions based on:
        <ul>
            <li>A keyed strategy (e.g., hash of the key determines partition)</li>
            <li>Or, if no key is provided, Kafka applies a round-robin approach to evenly distribute messages among
                partitions.</li>
        </ul>
        </p>
        <h2>What To Do After Messages Are Sent to Dead Letter Topic (DLT)?</h2>
        <p>
            After messages are sent to a Dead Letter Topic (DLT), you can take several actions to handle the failed
            messages:
        </p>
        <ol>
            <li><strong>Investigate the Cause:</strong> Analyze the messages in the DLT to understand why they failed.
                This may involve checking logs, error messages, and the original message content.</li>
            <li><strong>Implement Fixes:</strong> Once you identify the root cause of the failures, implement the
                necessary fixes in your application or data pipeline.</li>
            <li><strong>Reprocess Messages:</strong> After fixing the issues, you can reprocess the messages from the
                DLT. This may involve sending them back to the original topic or a different topic for reprocessing.
            </li>
            <li><strong>Monitor and Alert:</strong> Set up monitoring and alerting for your DLT to catch future failures
                early. This can help you respond more quickly and reduce the impact of failed messages.</li>
        </ol>
        <h2>Steps to Manage Dead Letter Topic Messages</h2>
        <ol>
            <li>
                <strong>Analyze the Failed Messages</strong>
                <p>Once a message lands in the DLT, the first step is to investigate why it failed:</p>
                <ul>
                    <li>Was there a data issue? (e.g., malformed JSON, invalid payload).</li>
                    <li>Was there a system failure? (e.g., timeout, unavailable resource).</li>
                    <li>Did the consumer's logic contain errors? (e.g., bugs in the processing code).</li>
                </ul>
                <p>üõ†Ô∏è How to Analyze:</p>
                <pre><code>kafka-console-consumer.sh --topic error-dlt --bootstrap-server localhost:9092 --from-beginning</code></pre>
                <p>Log and store the metadata for each failed message:</p>
                <ul>
                    <li>Original topic name.</li>
                    <li>Offset and partition number.</li>
                    <li>Error details (e.g., exception message).</li>
                </ul>
                <p>Example with Spring Boot Consumer:</p>
                <pre><code>@KafkaListener(topics = "error-dlt", groupId = "dlt-analyzer-group")
public void handleFailedMessages(ConsumerRecord&lt;String, String&gt; record) {
    System.out.println("DLT Message Key: " + record.key());
    System.out.println("DLT Message Value: " + record.value());
    System.out.println("Original Topic: " + record.topic());
    System.out.println("Partition: " + record.partition());
    System.out.println("Offset: " + record.offset());
}
</code></pre>
            </li>
            <li>
                <strong>Classify the Failures</strong>
                <p>After analyzing the messages, classify the root cause of the failures into categories such as:</p>
                <ul>
                    <li><strong>Recoverable Errors:</strong> Missing or temporary data dependencies (e.g., database
                        downtime). Timeout issues (e.g., calling an external API that failed temporarily). Processing
                        retries can often resolve these issues.</li>
                    <li><strong>Non-Recoverable Errors:</strong> Invalid input data (e.g., malformed JSON, missing
                        required fields). Business rule violations (e.g., payment amount exceeds limits). Requires
                        manual intervention or reprocessing with clean data.</li>
                </ul>
            </li>
            <li>
                <strong>Reprocess Failed Messages</strong>
                <p>Once you fix the issue (bug or configuration), the most common action is to reprocess the failed
                    messages from the DLT. This ensures no data is lost.</p>
                <p>üõ†Ô∏è Reprocessing Options:</p>
                <ul>
                    <li>
                        <strong>Option 1: Write a New Consumer for DLT</strong>
                        <p>Create a dedicated Spring Kafka Listener that reprocesses messages from the DLT.</p>
                        <pre><code>@KafkaListener(topics = "error-dlt", groupId = "dlt-reprocessor-group")
public void reprocessMessage(ConsumerRecord&lt;String, String&gt; record) {
    try {
        System.out.println("Reprocessing message: " + record.value());
        // Simulate your business logic here
        process(record.value());
        System.out.println("Message reprocessed successfully!");
    } catch (Exception e) {
        System.err.println("Failed again: " + e.getMessage());
        // Optionally send back to another DLT or archive permanently
    }
}

private void process(String message) {
    // Your business logic here
    if (message.contains("error")) {
        throw new RuntimeException("Simulated processing failure");
    }
}
</code></pre>
                    </li>
                    <li>
                        <strong>Option 2: Replay Messages to the Original Topic</strong>
                        <p>You can move messages from the DLT back to the original topic so they can be retried by the
                            original consumer after fixing the issue.</p>
                        <pre><code>kafka-console-consumer.sh --topic error-dlt --bootstrap-server localhost:9092 | \
kafka-console-producer.sh --topic input-topic --bootstrap-server localhost:9092</code></pre>
                        <p>Or automate this with a Spring Boot service:</p>
                        <pre><code>@KafkaListener(topics = "error-dlt", groupId = "dlt-replay-group")
public void replayMessageToOriginalTopic(ConsumerRecord&lt;String, String&gt; record,
                                         KafkaTemplate&lt;String, String&gt; kafkaTemplate) {
    final String originalTopic = "input-topic"; // The topic to resend the message to
    kafkaTemplate.send(originalTopic, record.key(), record.value());
    System.out.println("Replayed message to original topic: " + originalTopic);
}
</code></pre>
                    </li>
                    <li>
                        <strong>Option 3: Manual Reprocessing</strong>
                        <p>If the issue requires a data cleanup or manual analysis, export DLT messages and process them
                            manually:</p>
                        <pre><code>kafka-run-class.sh kafka.tools.DumpLogSegments --files &lt;log file path&gt;</code></pre>
                        <p>Fix data or clean invalid messages manually.</p>
                        <p>Inject cleaned messages back into the original topic:</p>
                        <pre><code>kafka-console-producer.sh --topic input-topic --bootstrap-server localhost:9092</code></pre>
                    </li>
                </ul>
            </li>
            <li>
                <strong>Archive Messages for Auditing or Recovery</strong>
                <p>For non-recoverable errors, you may want to archive failed messages for future audits or compliance
                    purposes.</p>
                <ul>
                    <li>
                        <strong>Dedicated Archival Topic:</strong> Move permanent failures from DLT to another Kafka
                        topic, like archived-errors:
                        <pre><code>@KafkaListener(topics = "error-dlt", groupId = "dlt-archive-group")
public void archiveMessage(ConsumerRecord&lt;String, String&gt; record, KafkaTemplate&lt;String, String&gt; template) {
    String archiveTopic = "archived-errors";
    template.send(archiveTopic, record.key(), record.value());
    System.out.println("Message archived to " + archiveTopic);
}
</code></pre>
                    </li>
                    <li>
                        <strong>Export to External Storage:</strong> Export failed messages to a durable storage
                        solution:
                        <ul>
                            <li>Amazon S3/Google Cloud Storage: Store messages as files.</li>
                            <li>Elasticsearch: Store for fast searching and indexing.</li>
                            <li>Relational Databases: Archive for long-term auditability.</li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li>
                <strong>Set Up Alerts for DLT Messages</strong>
                <p>Monitor the arrival of messages in the DLT and set up alerts when thresholds are exceeded. For
                    example:</p>
                <ul>
                    <li>Use Prometheus + Grafana to track message counts in error-dlt.</li>
                    <li>Integrate Alertmanager to notify your team (via Slack, email, etc.) if too many messages land in
                        the DLT.</li>
                </ul>
                <p>Example Prometheus Query:</p>
                <pre><code>sum(rate(kafka_topic_partition_dlt_message_count[5m]))</code></pre>
            </li>
            <li>
                <strong>Prevent Future Failures</strong>
                <p>After resolving the immediate issue, take steps to prevent similar failures in the future:</p>
                <ul>
                    <li><strong>Data Validation Before Publishing:</strong> Validate messages at the producer level to
                        avoid sending invalid data. Example: Add validation logic in Spring Boot Producer.
                        <pre><code>if (!isValid(message)) {
    throw new IllegalArgumentException("Invalid message format");
}
kafkaTemplate.send("input-topic", message);
</code></pre>
                    </li>
                    <li><strong>Robust Retry Mechanisms:</strong> Configure retries before messages are sent to the DLT.
                        <pre><code>spring.kafka.listener.retry.max-attempts=3
spring.kafka.listener.backoff.interval=1000ms
</code></pre>
                    </li>
                    <li><strong>Improve Logging and Observability:</strong> Add detailed logs for failed messages.
                        Implement observability tools like OpenTelemetry for distributed tracing.</li>
                    <li><strong>Automate Data Sanitization:</strong> Clean up known problematic data patterns via
                        automated transformations before publishing.</li>
                </ul>
            </li>
        </ol>
        <h3>Suggested Workflow for Managing DLT Messages</h3>
        <ol>
            <li>Monitor the DLT: Set up alerting to detect when messages arrive in the DLT.</li>
            <li>Investigate &amp; Classify Errors: Analyze whether the errors are recoverable or non-recoverable.</li>
            <li>Reprocess Recoverable Messages: Replay messages to the original topic or handle them programmatically.
            </li>
            <li>Archive Non-Recoverable Messages: Export messages to a safe location for later inspection/auditing.</li>
            <li>Fix Root Cause: Address the bug or misconfiguration to avoid similar errors in the future.</li>
        </ol>
    </main>
    <footer>
        &copy; 2025 Rafiqul Islam. All rights reserved.
    </footer>
</body>

</html>